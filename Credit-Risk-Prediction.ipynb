{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Prediction\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "### 1. [Introduction](#i)\n",
    "\n",
    "### 2. [Exploratory data analysis](#eda)\n",
    "   \n",
    "   - #### [Loading the dataset](#eda_1)\n",
    "   \n",
    "   - #### [Univariate analysis](#eda_2)\n",
    "   \n",
    "   - #### [Bivariate analysis](#eda_3)\n",
    "   \n",
    "### 3. [Feature engineering](#fe)\n",
    "\n",
    "### 4. [Model selection](#ms)\n",
    "   \n",
    "   - #### [Unbalanced dataset](#ms_1)\n",
    "   \n",
    "   - #### [Balanced dataset](#ms_2)\n",
    "   \n",
    "### 5. [Feature selection](#fs)\n",
    "   \n",
    "### 6. [Dataset splitting](#ds)\n",
    "\n",
    "### 7. [Hyperparameter optimization](#ho)\n",
    "   \n",
    "   - #### [Fine-tuning the XGBoost model](#ho_1)\n",
    "   \n",
    "   - #### [Fine-tuning the Random Forest model](#ho_2)\n",
    "   \n",
    "   - #### [Fine-tuning the LightGBM model](#ho_3)\n",
    "   \n",
    "   - #### [Fine-tuning the Gradient Boosting model](#ho_4)\n",
    "\n",
    "### 8. [Ensemble model](#em)\n",
    "   \n",
    "   - #### [Individual model evaluation](#em_1)\n",
    "   \n",
    "   - #### [Ensemble model evaluation](#em_2)\n",
    "   \n",
    "   - #### [Ensemble model analysis](#em_3)\n",
    "   \n",
    "### 9. [Model fairness analysis](#mfa)\n",
    "\n",
    "### 10. [Conclusion](#c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a name=\"i\"></a>\n",
    "\n",
    "The aim of this project is to assist banks in assessing the credit risk of their loan applicants.\n",
    "\n",
    "A bank makes the decision whether or not to grant an applicant a loan based on their profile. This exposes the bank to two types of risk:\n",
    "\n",
    "- If the bank declines the application of a person with a good credit risk, then it loses business in the form of the interests that the client were going to pay.\n",
    "\n",
    "- If the bank approves the application of a person with a bad credit risk, then it loses the entirety of the loaned sum *when* that client defaults.\n",
    " \n",
    "Our main goal is not only to minimize the risk while maximizing the profit of the bank but to also consider the fairness of our final model. This is relevant today because some of the existing credit scoring models seem to [take gender into significant consideration.](https://www.nytimes.com/2019/11/10/business/Apple-credit-card-investigation.html)\n",
    "\n",
    "\n",
    "We will be using the German Credit Dataset available [here](https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)).\n",
    "\n",
    "This dataset contains information about 1000 credit applicants. Each applicant is classified as either **Good** or **Bad** as a reflection of their credit risk:\n",
    "- A **Good** applicant has a low credit risk and is thus worthy of a loan.\n",
    "- A **Bad** applicant has a high credit risk and should not be granted a loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the libraries that we will be using, they can be installed by running the follwoing code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imbalanced-learn>=0.6.1 lightgbm>=2.3.1 matplotlib>=3.1.2 numpy>=1.17.4 pandas>=0.25.3 plotly>=4.3.0 scikit-optimize>=0.5.2 seaborn>=0.9.0 scikit-learn>=0.22 xgboost>=0.90 fairlearn>=0.4.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, make_scorer, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "from skopt.space import Real, Integer\n",
    "from skopt import gp_minimize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections as mc\n",
    "import seaborn as sns\n",
    "\n",
    "# Custom created plotting functions\n",
    "from plotting_functions import plot_variables, plot_pies, plot_importance_metrics, plot_confusion_matrix, plot_ROC, plot_votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis <a name=\"eda\"></a>\n",
    "\n",
    "## Loading the dataset  <a name=\"eda_1\"></a>\n",
    "We start by taking a look our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"german.data\", delimiter=' ', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the categories have been coded but we will se the supplied dataset description to decode them in the following cell in order to get a good grasp of what the different variables and categories are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"Existing checking account   Duration in month   Credit history   Purpose   Credit amount   Savings account/bonds   Present employment since   Installment rate   Personal status   Other debtors   Present residence since   Property   Age   Other installment plans   Housing   Existing credits   Job   Number of people being liable   Telephone   Foreign worker   Score\"\n",
    "categories = {\"Existing checking account\": {\"A11\": \" ... < 0 DM\", \"A12\": \"0 <= ... < 200 DM\", \"A13\": \" ... >= 200 DM /salary assignments for at least 1 year\", \"A14\": \"no checking account\"},\n",
    "              \"Credit history\": {\"A30\": \"no credits taken/all credits paid back duly\", \"A31\": \"all credits at this bank paid back duly\", \"A32\": \"existing credits paid back duly till now\", \"A33\": \"delay in paying off in the past\", \"A34\": \"critical account/other credits existing (not at this bank)\"},\n",
    "              \"Purpose\": {\"A40\": \"car (new)\", \"A41\": \"car (used)\", \"A42\": \"furniture/equipment\", \"A43\": \"radio/television\", \"A44\": \"domestic appliances\", \"A45\": \"repairs\", \"A46\": \"education\", \"A48\": \"retraining\", \"A49\": \"business\", \"A410\": \"others\"},\n",
    "              \"Savings account/bonds\": {\"A61\": \"... < 100 DM\", \"A62\": \" 100 <= ... < 500 DM\", \"A63\": \" 500 <= ... < 1000 DM\", \"A64\": \" ... >= 1000 DM\", \"A65\": \" unknown/ no savings account\"},\n",
    "              \"Present employment since\": {\"A71\": \"unemployed\", \"A72\": \" ... < 1 year\", \"A73\": \"1 <= ... < 4 years \", \"A74\": \"4 <= ... < 7 years\", \"A75\": \" ... >= 7 years\"},\n",
    "              \"Personal status\": {\"A91\": \"divorced/separated\", \"A92\": \"undefined\", \"A93\": \"single\", \"A94\": \"married/widowed\"},\n",
    "              \"Gender\": {\"A91\": \"male\", \"A92\": \"female\", \"A93\": \"male\", \"A94\": \"male\"},\n",
    "              \"Other debtors\": {\"A101\": \"none\", \"A102\": \"co-applicant\", \"A103\": \"guarantor\"},\n",
    "              \"Property\": {\"A121\": \"real estate\", \"A122\": \"building society savings agreement/life insurance\", \"A123\": \"car or other\", \"A124\": \"unknown / no property\"},\n",
    "              \"Other installment plans\": {\"A141\": \"bank\", \"A142\": \"stores\", \"A143\": \"none\"},\n",
    "              \"Housing\": {\"A151\": \"rent\", \"A152\": \"own\", \"A153\": \"for free\"},\n",
    "              \"Job\": {\"A171\": \"unemployed/ unskilled - non-resident\", \"A172\": \"unskilled - resident\", \"A173\": \"skilled employee / official\", \"A174\": \"management/ self-employed/highly qualified employee/ officer\"},\n",
    "              \"Telephone\": {\"A191\": \"no\", \"A192\": \"yes\"},\n",
    "              \"Foreign worker\": {\"A201\": \"yes\", \"A202\": \"no\"}}\n",
    "\n",
    "data = pd.read_csv(\"german.data\", delimiter=' ', names=header.split('   '))\n",
    "\n",
    "# Add a Gender variable to separate it from Personal status\n",
    "data.insert(8, \"Gender\", data[\"Personal status\"])\n",
    "\n",
    "# Encode categorical variables\n",
    "for category in categories:\n",
    "    data[category] = data[category].map(categories[category])\n",
    "\n",
    "# Encode score\n",
    "data[\"Score\"] = data[\"Score\"].map({1: \"good\", 2: \"bad\"})\n",
    "\n",
    "n, p = data.shape\n",
    "print(\"We have \", n, \" observations of \", p, \"variables\")\n",
    "print(\"Number of missing values : \",\n",
    "      len(data[data.isnull().isin([True])].stack()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than the convoluted category coding scheme, the dataset is well structured and has no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate analysis <a name=\"eda_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by looking at the distribution of the categorical and discrete variables in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_disc_vars = [var for var in list(data) if var not in (\"Age\", \"Duration in month\", \"Credit amount\")]\n",
    "plot_variables(cat_disc_vars, 0, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not notice any abnormal distributions.\n",
    "\n",
    "However, we note by looking at the `Score` variable that 30% of the applicants are classified as Bad and 70% as Good. We will most likely have to correct this imbalance in order to improve the performance of our future models.\n",
    "\n",
    "We also note by looking at the `Gender` variable that only 31% of the applicants are women which is differnet from the 50%/50% split that we would expect to see. This may indicate another imbalance problem that, this time, relates to the fairness of our future models.\n",
    "\n",
    "We now look at the distribution of the continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variables([\"Age\", \"Credit amount\", \"Duration in month\"], 1, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We notice that the three variables are positively skewed with a long tail indicating, respectively, that most applicants are young, have received a small loan, and for a duration that is less than 24 months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate analysis <a name=\"eda_3\"></a>\n",
    "We now look at the distribution of the `Score` variable across other categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [var for var in list(data) if var not in (\"Age\", \"Duration in month\", \"Credit amount\", \"Purpose\", \"Score\")]\n",
    "plot_pies(labels, data, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the scores of most variables is close to what we can logically expect.\n",
    "A few variables like `Credit history` and `Job` seem counterintuitive, this may be due to the category labels might be mis-specified.\n",
    "\n",
    "We can also note that variables like `Number of people being liable`, `Present residence since` and `Telephone` have a low variance across the different score classes which may indicate their poor predictive power when compared to variables such as `Existing checking account` or `Credit history` etc.\n",
    "\n",
    "We now look at how the continuos variables are distributed for each scrore class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plot_variables([\"Age\", \"Credit amount\", \"Duration in month\"], 2, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplots show that applications with an older applicant age, a lower credit amount and a shorter loan duration are less likely to be bad.\n",
    "\n",
    "This is quite logical and depsite the differences between the two boxplots not being very large, we can expect these three variables to have a significant predictive power.\n",
    "\n",
    "# Feature engineering <a name=\"fe\"></a>\n",
    "\n",
    "In this section we modify our data by adding new dummy variables and transforming some of the existing ones.\n",
    "\n",
    "We start by log-transforming the `Credit amount` variable in order to reduce the scale difference across the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Credit amount\"] = np.log(data[\"Credit amount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the categorical variables are actually ordered. We encode them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_categories = {\"Existing checking account\": {\" ... < 0 DM\": 0, \"0 <= ... < 200 DM\": 2, \" ... >= 200 DM /salary assignments for at least 1 year\": 3, \"no checking account\": 1},\n",
    "                      \"Savings account/bonds\": {\"... < 100 DM\": 1, \" 100 <= ... < 500 DM\": 2, \" 500 <= ... < 1000 DM\": 3, \" ... >= 1000 DM\": 4, \" unknown/ no savings account\": 0},\n",
    "                      \"Present employment since\": {\"unemployed\": 0, \" ... < 1 year\": 1, \"1 <= ... < 4 years \": 2, \"4 <= ... < 7 years\": 3, \" ... >= 7 years\": 4},\n",
    "                      \"Property\": {\"real estate\": 3, \"building society savings agreement/life insurance\": 2, \"car or other\": 1, \"unknown / no property\": 0},\n",
    "                      \"Housing\": {\"rent\": 0, \"own\": 2, \"for free\": 1},\n",
    "                      \"Job\": {\"unemployed/ unskilled - non-resident\": 0, \"unskilled - resident\": 1, \"skilled employee / official\": 3, \"management/ self-employed/highly qualified employee/ officer\": 2},\n",
    "                      \"Score\": {\"bad\": 0, \"good\": 1}}\n",
    "for category in ordinal_categories:\n",
    "    data[category] = data[category].map(ordinal_categories[category]).astype(\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ordinal variables correctly ordered we can look at the correlation matrix of some our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr()\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the correlation coefficients are not particularly high, indicating the lack of multicollinearity. This eliminates the need for exploring certain dimension reduction techniques such as PCA.\n",
    "\n",
    "With the ordinal variables correctly encoded, we now one-hot encode the remaining categorical variables. This creates dummy variables for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, drop_first=True)\n",
    "# Drop redundant variable\n",
    "data.drop('Personal status_undefined', axis=1, inplace=True) \n",
    "X = data.drop('Score', axis=1)\n",
    "y = data['Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection <a name=\"ms\"></a>\n",
    "In this section we test various baseline models. We will use 5-fold crossvalidation to determine the best candidate models moving forward. \"Best\" is defined as having a low cost which is defined in the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom cost function for our problem\n",
    "def cost(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    # we normalize the interest from a good credit to 1, and the loss from a credit default to 5\n",
    "    max_gain = (tp + fn) * 1 # maximum gain = number of actual good credit applicants\n",
    "    # TP generate an interest of 1, FP results in a loss of 5, and FN cause an opportunity cost of 1\n",
    "    actual_gain = (tn * 0 + tp * 1 - fp * 5 - fn * 1)\n",
    "    cost = max_gain - actual_gain\n",
    "    return cost/max_gain  # Normalization \n",
    "\n",
    "costMetric = make_scorer(cost)\n",
    "metrics = {'Cost': costMetric}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbalanced dataset <a name=\"ms_1\"></a>\n",
    "We start by testing the different models on the unbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "models = {'Logistic Regression': LogisticRegression(solver=\"liblinear\"),\n",
    "          'KNN': KNeighborsClassifier(),\n",
    "          'Random Forest': RandomForestClassifier(),\n",
    "          'SVM': SVC(),\n",
    "          'XGBoost': XGBClassifier(),\n",
    "          'Adaboost': AdaBoostClassifier(),\n",
    "          'LightGBM': LGBMClassifier(),\n",
    "          'Gradient Boosting': GradientBoostingClassifier()}\n",
    "\n",
    "scoringMetric = \"Cost\"\n",
    "results = pd.DataFrame(columns=models)\n",
    "for model in models:\n",
    "    kfold = KFold(shuffle=True, n_splits=5, random_state=0)\n",
    "    cvResults = cross_val_score(models[model], X, y, cv=kfold, scoring=metrics[scoringMetric])\n",
    "    results[model] = cvResults\n",
    "\n",
    "bxplot = sns.boxplot(x=\"Model\", y=scoringMetric, data=results.melt(var_name=\"Model\", value_name=scoringMetric))\n",
    "_ = bxplot.set_xticklabels(bxplot.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced dataset <a name=\"ms_2\"></a>\n",
    "We now test the same models on the balanced dataset\n",
    "\n",
    "To balance the data, we use an oversampling method because the number of observations is limited and trying an undersampling method resulted in worse performance.\n",
    "\n",
    "We choose to use SMOTE to generate examples for the underrepresented class. This algorithm selects an observation $X$ from this class, computes its closest $k$ neighbors (all classes included), selects one random neighbor $N$ from them, then generates a new observation defined by the barycenter of $X$ and $N$ using random weights.\n",
    "\n",
    "We make sure to balance each fold and not the entire dataset in order to not inflate our scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'Logistic Regression': LogisticRegression(solver=\"liblinear\"),\n",
    "          'KNN': KNeighborsClassifier(),\n",
    "          'Random Forest': RandomForestClassifier(),\n",
    "          'SVM': SVC(),\n",
    "          'XGBoost': XGBClassifier(),\n",
    "          'Adaboost': AdaBoostClassifier(),\n",
    "          'LightGBM': LGBMClassifier(),\n",
    "          'Gradient Boosting': GradientBoostingClassifier()}\n",
    "\n",
    "scoringMetric = \"Cost\"\n",
    "results = pd.DataFrame(columns=models)\n",
    "for model in models:\n",
    "    classifier = models[model]\n",
    "    kfold = KFold(shuffle=True, n_splits=5, random_state=0)\n",
    "    cvResults = []\n",
    "    # Use SMOTE to balance each fold\n",
    "    for train_idx, test_idx, in kfold.split(X, y):\n",
    "        X_train, y_train = X.iloc[train_idx], y[train_idx]\n",
    "        X_test, y_test = X.iloc[test_idx], y[test_idx]\n",
    "        X_train, y_train = SMOTE().fit_sample(X_train, y_train)\n",
    "        X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        cvResults.append(cost(y_test, y_pred))\n",
    "    results[model] = cvResults\n",
    "\n",
    "bxplot = sns.boxplot(x=\"Model\", y=scoringMetric, data=results.melt(var_name=\"Model\", value_name=scoringMetric))\n",
    "_ = bxplot.set_xticklabels(bxplot.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that balancing the training dataset results in better results on the test set as evident by the lower costs. Some models (Random Forest, XGBoost, LightGBM and Gradient Boosting) have lower costs than others so we select these models moving forward\n",
    "\n",
    "We will also continue working with the balanced dataset henceforward.\n",
    "\n",
    "# Dataset splitting <a name=\"ds\"></a>\n",
    "\n",
    "We identified the models having the most potential, we now split our dataset in a stratified fashion into Train, Validation and Test sets.\n",
    "We make sure not to balance our Test set in order to not bias our evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.15,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=y)\n",
    "# Rebalance training set\n",
    "X_train, y_train = SMOTE(random_state=0).fit_sample(X_train, y_train)\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)             \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                    test_size=0.15,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Feature selection <a name=\"fs\"></a>\n",
    "\n",
    "We have multiple variables that seem unimportant, this section aims is to identify and remove these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "print(\"Number of features: \", X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We will evaluate which features according to a number of metrics: Chi2, R2, XGBoost importance and RF importance.\n",
    "Each metric has its own merits as well as its caveats, which is why we average the four of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbcls = XGBClassifier(random_state=0)  # XGB\n",
    "xgbcls.fit(X_train, y_train)\n",
    "feat_importance = pd.DataFrame()\n",
    "feat_importance[\"XGB\"] = xgbcls.feature_importances_\n",
    "rfcls = RandomForestClassifier(random_state=0)  # RF\n",
    "rfcls.fit(X_train, y_train)\n",
    "feat_importance[\"RF\"] = rfcls.feature_importances_\n",
    "feat_importance[\"Chi2\"] = [(1-x)/10 for x in chi2(X_train, y_train)[1]]  # Chi2\n",
    "# The metrics are scaled to be of the same order of magnitude\n",
    "corr = [] # R2\n",
    "for col in X_train.columns:\n",
    "        corr.append(np.corrcoef(X_train[col], y_train)[0, 1]**2)\n",
    "feat_importance[\"Correlation\"] = corr\n",
    "feat_importance[\"Average\"] = feat_importance.mean(axis=1)\n",
    "sorted_idx = feat_importance[\"Average\"].argsort()\n",
    "# Plotting the metrics\n",
    "plot_importance_metrics(feat_importance, X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Most variables appear potentially useful, we will only remove the 5 least important ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "sorted_idx = feat_importance[\"Average\"].argsort()\n",
    "X_train.drop(X_train.columns[sorted_idx[:5]], axis=1, inplace=True)\n",
    "X_val.drop(X_val.columns[sorted_idx[:5]], axis=1, inplace=True)\n",
    "X_test.drop(X_test.columns[sorted_idx[:5]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization <a name=\"ho\"></a>\n",
    "\n",
    "In this section we improve our selected models by tuning their hyperparameters using Bayesian optimization.\n",
    "This technique, in a nutshell, is an efficient sequential strategy for global optimization of evaluation-expensive black-box functions that doesn't require derivatives.\n",
    "It treats the objective as a random function and after gathering the function evaluations, it updates the posterior distribution over the objective function which will be used to determine the query points for the next iteration.\n",
    "In this case the objective function to minimize is our custom-defined cost function, and the optimization space we will use is the cartesian product of intervals containing the default values of each hyperparameter.\n",
    "\n",
    "In order to avoid biasing our final evaluations we are using a validation set for this tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining the `optimise` function that is used to find the optimal paremeters in a given search space for a given number of iterations. \n",
    "\n",
    "We also define the `evaluate_tuning` function which is used to compare the confusion matrices of model before after the tuning procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def optimise(model, space, X_train, y_train, X_val, y_val, n_calls = 20):\n",
    "    def model_assessment(args, X_train, y_train, X_val, y_val):\n",
    "        # fit then evaluate the model on validation data then return its score\n",
    "        params = {curr_model_hyper_params[i]: args[i] for i, j in enumerate(curr_model_hyper_params)}\n",
    "        copy.set_params(**params)\n",
    "        fitted_model = copy.fit(X_train, y_train)\n",
    "        val_predictions = fitted_model.predict(X_val)\n",
    "        val_score = cost(y_val, val_predictions)\n",
    "        return(val_score)\n",
    "    copy = deepcopy(model)\n",
    "    curr_model_hyper_params = [i.name for i in space]\n",
    "    # define the objective function as the cost given by the model on validation data\n",
    "    objective_function = partial(model_assessment, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)\n",
    "    # use bayesian optimization to minimize the objective function\n",
    "    results = gp_minimize(objective_function, space, base_estimator=None, n_calls=n_calls, n_random_starts=20, random_state=0)\n",
    "    # get the parameters that minimize the cost\n",
    "    bestParams = dict(zip(curr_model_hyper_params, results.x))\n",
    "    return copy.set_params(**bestParams)\n",
    "\n",
    "def evaluate_tuning(untuned, tuned, name, X_train, y_train, X_test, y_test):\n",
    "    # fit the tuned and untuned models to the train data then plot their confusion matrices\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(16, 6))\n",
    "    untuned.fit(X_train, y_train)\n",
    "    tuned.fit(X_train, y_train)\n",
    "    cm_untuned = confusion_matrix(y_test, untuned.predict(X_test))\n",
    "    cm_tuned = confusion_matrix(y_test, tuned.predict(X_test))\n",
    "    cost_untuned = cost(y_test, untuned.predict(X_test))\n",
    "    cost_tuned = cost(y_test, tuned.predict(X_test))\n",
    "    plot_confusion_matrix(cm_untuned, ax=axes[0], title=f\"Untuned {name} (Cost:{cost_untuned:0.3f})\")\n",
    "    plot_confusion_matrix(cm_tuned, ax=axes[1], title=f\"Tuned {name} (Cost:{cost_tuned:0.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the XGBoost model <a name=\"ho_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "space = [Real(0.001, 0.3, name=\"gamma\"),\n",
    "         Real(0.05, 0.8, name=\"learning_rate\"),\n",
    "         Real(0.6, 5, name=\"max_delta_step\"),\n",
    "         Integer(3, 10, name=\"max_depth\"),\n",
    "         Real(0.2, 5, name=\"min_child_weight\"),\n",
    "         Integer(50, 200, name=\"n_estimators\")]\n",
    "\n",
    "xgbcls = XGBClassifier(random_state=0)\n",
    "bestXGBoost = optimise(xgbcls, space, X_train, y_train, X_val, y_val,50)\n",
    "evaluate_tuning(xgbcls, bestXGBoost, \"XGBoost\",X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the tuned model has a better confusion matrix and a lower cost.\n",
    "## Fine-tuning the Random Forest model <a name=\"ho_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "space = [Integer(2, 50, name=\"min_samples_split\"),\n",
    "    Integer(1, 50, name=\"min_samples_leaf\"),\n",
    "    Integer(3, 200, name=\"max_depth\"),\n",
    "    Integer(100, 300, name=\"n_estimators\")]\n",
    "rfcls = RandomForestClassifier( random_state=0)\n",
    "bestRF = optimise(rfcls, space, X_train, y_train, X_val, y_val, 40)\n",
    "evaluate_tuning(rfcls, bestRF, \"Random Forest\",X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the tuned model has a better confusion matrix and a lower cost.\n",
    "\n",
    "## Fine-tuning the LightGBM model <a name=\"ho_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "space = [Real(0.04, 0.7, name=\"learning_rate\"),\n",
    "         Integer(0, 15, name=\"max_depth\"),\n",
    "         Integer(10, 50, name=\"num_leaves\"),\n",
    "         Real(0.001, 3, name=\"min_child_weight\"),\n",
    "         Integer(50, 300, name=\"n_estimators\")]\n",
    "lgbmcls = LGBMClassifier(random_state=0)\n",
    "bestLGBM = optimise(lgbmcls, space, X_train, y_train, X_val, y_val, 50)\n",
    "evaluate_tuning(lgbmcls, bestLGBM, \"LightGBM\",X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the tuned model has a better confusion matrix and a lower cost.\n",
    "## Fine-tuning the Gradient Boosting model <a name=\"ho_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = [Integer(3, 10, name=\"min_samples_split\"),\n",
    "         Integer(5, 7, name=\"min_samples_leaf\"),\n",
    "         Integer(5, 8, name=\"max_depth\"),\n",
    "         Real(0.8, 0.9, name=\"subsample\"),\n",
    "         Integer(10, 15, name=\"max_features\"),\n",
    "         Integer(50, 100, name=\"n_estimators\")]\n",
    "gbcls = GradientBoostingClassifier(random_state=0)\n",
    "bestGBoost = optimise(gbcls, space, X_train, y_train, X_val, y_val, 50)\n",
    "evaluate_tuning(gbcls, bestGBoost, \"Gradient Boosting\",X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the tuned model has a better confusion matrix and a lower cost.\n",
    "On average, we can see that tuning the models resulted in approximately 10% fewer false classifications\n",
    "\n",
    "# Ensemble model <a name=\"em\"></a>\n",
    "\n",
    "## Individual model evaluation <a name=\"em_1\"></a>\n",
    "\n",
    "We start by looking at the different ROC curves of the models, we can see that they all have a similar performance. We can't choose a single model because none of them stands out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "classifiers = (bestRF,\n",
    "               bestXGBoost,\n",
    "               bestLGBM,\n",
    "               bestGBoost)\n",
    "result_table = pd.DataFrame(columns=['classifier', 'fpr', 'tpr', 'auc', 'cm', 'cost'])\n",
    "\n",
    "for cls in classifiers:\n",
    "    model = cls.fit(X_train, y_train)\n",
    "    yproba = model.predict_proba(X_test)[::, 1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test,  yproba)\n",
    "    auc = roc_auc_score(y_test, yproba)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cst = cost(y_test, y_pred)\n",
    "    result_table = result_table.append({'classifier': cls.__class__.__name__,\n",
    "                                        'fpr': fpr,\n",
    "                                        'tpr': tpr,\n",
    "                                        'auc': auc,\n",
    "                                        'cm': cm,\n",
    "                                        'cost': cst}, ignore_index=True)\n",
    "result_table.set_index('classifier', inplace=True)\n",
    "plot_ROC(result_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ensemble model evaluation <a name=\"em_2\"></a>\n",
    "We now create an ensemble out of all these models and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('Random Forest', bestRF),\n",
    "              ('XGBoost', bestXGBoost),\n",
    "              ('LightGBM', bestLGBM),\n",
    "              ('gradboost', bestGBoost)]\n",
    "result_table.reset_index('classifier', inplace=True)\n",
    "\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "yproba = ensemble.predict_proba(X_test)[::, 1]\n",
    "yproba = pd.Series(yproba)\n",
    "yproba.index = y_test.index\n",
    "y_pred = ensemble.predict(X_test)\n",
    "y_pred = pd.Series(y_pred)\n",
    "y_pred.index = y_test.index\n",
    "fpr, tpr, _ = roc_curve(y_test,  yproba)\n",
    "auc = roc_auc_score(y_test, yproba)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cst = cost(y_test, y_pred)\n",
    "result_table = result_table.append({'classifier': ensemble.__class__.__name__,\n",
    "                                    'fpr': fpr,\n",
    "                                    'tpr': tpr,\n",
    "                                    'auc': auc,\n",
    "                                    'cm': cm,\n",
    "                                    'cost': cst}, ignore_index=True)\n",
    "result_table.set_index('classifier', inplace=True)\n",
    "plot_ROC(result_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We can see that the ensemble model has a higher AUC than each individual model and a low cost. We can also look at its confusion matrix which is indeed better than each of all the previous models'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(result_table['cm']['VotingClassifier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble model analysis <a name=\"em_3\"></a>\n",
    "\n",
    "We now look at the ensemble model in greater detail. \n",
    "\n",
    "Since most of our individuals models are based on boosting and bagging and are thus rather stable, we decided to choose a soft voting ensembling technique in which the probability vectors for each predicted class (for all models) are summed up  and averaged.\n",
    "\n",
    "The individual models are not always in sync. For example we visualize below the contribution of each classifier to the final predicted class of a random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = [c.predict_proba(X_test) for c in (bestRF, bestXGBoost, bestLGBM, bestGBoost, ensemble)]\n",
    "# get class probabilities for a sample\n",
    "bad = [pr[59, 0] for pr in probas]\n",
    "good = [pr[59, 1] for pr in probas]\n",
    "plot_votes(bad, good, result_table.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at which features are most important for the model by looking at the permutation feature importance. This metric is defined as the decrease in a model score when a single feature value is randomly shuffled. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "result = permutation_importance(ensemble, X_train, y_train, n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,10), dpi=200)\n",
    "ax.boxplot(result.importances[sorted_idx].T,\n",
    "           vert=False, labels=X_train.columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importance (train set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most important features relate to the credit amount, duration and the background of the applicant. Fortunately we can see that the gender of the applicant (the dummy variable `Gender_male`) ranks low on the importance scale which may be inidicative of the lack of discrimination based on gender. We will however investigate this further in the following section.\n",
    "\n",
    "# Model fairness analysis <a name=\"mfa\"></a>\n",
    "\n",
    "We start by defining the `evaluate_groups` function which is used to calculate different metrics for each sub-group of applicants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demographic Parity** states that the proportion of each gender should receive the positive outcome at equal rates, i.e.\n",
    "\n",
    "$$P(Good|Gender=Male) = P(Good|Gender=Female)$$\n",
    "\n",
    "In practise, we may not require for the difference in Positive Rates to equal 0, but we will aim to minimise this gap.\n",
    "\n",
    "**Equalised Odds** requires the positive outcome to be independent of the gender, conditional on the actual reference class, i.e.\n",
    "\n",
    "$$P(Prediction=Good|Gender=Male,Reference=class) = P(Prediction=Good|Gender=Female, Reference=class)\\\\ class \\in \\{Good, Bad\\}$$\n",
    "\n",
    "Based on the confusion matrix, we may not require the True Positive Rate and False Positive Rate to be the same for each gender but we will aim to minimise both gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def evaluate_groups(y_test, y_pred, indexes, names, yproba=pd.Series()):\n",
    "    group_results = pd.DataFrame(columns=['group', 'fpr', 'tpr', 'auc', 'cm', 'cost'])\n",
    "\n",
    "    for group_idx, group_name in zip(indexes, names):\n",
    "        y_test_group = y_test[group_idx]\n",
    "        y_pred_group = y_pred[group_idx]\n",
    "        if not yproba.empty:\n",
    "            yproba_group = yproba[group_idx]\n",
    "            fpr, tpr, _ = roc_curve(y_test_group,  yproba_group)\n",
    "            auc = roc_auc_score(y_test_group, yproba_group)\n",
    "        else:\n",
    "            fpr, tpr, auc = 0, 0, 0\n",
    "        cm = confusion_matrix(y_test_group, y_pred_group)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        cst = cost(y_test_group, y_pred_group)\n",
    "        eo = tp/(tp + fn)\n",
    "        pdp = (tp + fp) / (tp + fp + tn + fn)\n",
    "        group_results = group_results.append({'group': group_name,\n",
    "                                            'fpr': fpr,\n",
    "                                            'tpr': tpr,\n",
    "                                            'auc': auc,\n",
    "                                            'cm': cm,\n",
    "                                            'cost': cst,\n",
    "                                            'equalized_odds': eo,\n",
    "                                            'prop_demo_parity': pdp}, ignore_index=True)\n",
    "    group_results.set_index('group', inplace=True)\n",
    "    return group_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_idx = X_test[X_test[\"Gender_male\"]==1].index.values\n",
    "female_idx = X_test[X_test[\"Gender_male\"]==0].index.values\n",
    "group_evaluations = evaluate_groups(y_test, y_pred, [male_idx, female_idx], [\"Male\", \"Female\"], yproba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the confusuion matrix of each gender as well as their equalized odds and proportional demographic parity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(16, 6))\n",
    "plot_confusion_matrix(group_evaluations[\"cm\"][0], ax=axes[0], title=f'Male (Cost: {group_evaluations[\"cost\"][0]:0.3f})')\n",
    "plot_confusion_matrix(group_evaluations[\"cm\"][1], ax=axes[1], title=f'Female (Cost:{group_evaluations[\"cost\"][1]:0.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "print(group_evaluations[[\"equalized_odds\", \"prop_demo_parity\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a wrapper class for our ensemble model. This new classifier outputs probabilites instead of class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class EnsembleClassifier:\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.estimator.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probabilities = self.estimator.predict_proba(X)[:,1]\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of outputing a certain class because its probability is higher than 0.5, we choose a different threshold for each gender. The thresholds are chosen in a way to minimize the gap in Demographic Parity, which will in return reduce the gap in Equalized Odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ensmebleWrapper = EnsembleClassifier(ensemble)\n",
    "postprocessedEstimator = ThresholdOptimizer(\n",
    "    unconstrained_predictor=ensmebleWrapper,\n",
    "    constraints=\"demographic_parity\")\n",
    "\n",
    "postprocessedEstimator._plot = True\n",
    "postprocessedEstimator.fit(X_train, y_train, sensitive_features=X_train[\"Gender_male\"])\n",
    "\n",
    "ypred_new = postprocessedEstimator.predict(X_test, sensitive_features=X_test[\"Gender_male\"])\n",
    "ypred_new = pd.Series(ypred_new)\n",
    "ypred_new.index = y_test.index\n",
    "\n",
    "group_evaluations = evaluate_groups(y_test, ypred_new, [male_idx, female_idx], [\"Male\", \"Female\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the confusion matrix of each gender and we can see that the performance of the model has degraded. The model classified women as good more often which results in more False Positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(16, 6))\n",
    "plot_confusion_matrix(group_evaluations[\"cm\"][0], ax=axes[0], title=f'Male (Cost: {group_evaluations[\"cost\"][0]:0.3f})')\n",
    "plot_confusion_matrix(group_evaluations[\"cm\"][1], ax=axes[1], title=f'Female (Cost:{group_evaluations[\"cost\"][1]:0.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite this degraded performance, we can see that the model is more fair towards both genders now. The gaps in both metrics are significantly lower than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(group_evaluations[[\"equalized_odds\", \"prop_demo_parity\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion <a name=\"c\"></a>\n",
    "\n",
    "\n",
    "In conclusion, machine learning seems to offer some decent insight to credit risk assessment. Achieving an AUC of ~84% on a problem that is difficult for humans and rule-based systems proves so. \n",
    "\n",
    "Our ensembling technique resulted in an improved performance and since it is based on probabilites, the model can be analyzed and improved, as evident by our ex-post fairness enhancing technique.\n",
    "\n",
    "Using SciKit-Learn means that our model can be easily fitted and applied to new data. The model achieves results good enough to be used in a business environment, although training it on more data should make it perform better. In such a use case, online learning isn't necessary since creditworthiness measure doesn't change over time.\n",
    "The final model is scalable and can fit to 10-100 times bigger datasets quickly. Re-tuning the model takes a bit more time but it remains reasonable in a batch-learning context.\n",
    "\n",
    "In regards to fairness it is evident that there is an arbitrage between having the best model performance (hence the most profit) and having more fairness. Nonetheless it is definitely possible to build a model that does not discriminate from the ground up.\n",
    "\n",
    "Other techniques such as neural networks might have some added value but we doubt that they will be effective in this case given the small size of our dataset."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
